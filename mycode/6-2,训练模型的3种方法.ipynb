{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-2,训练模型的3种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的训练主要有内置fit方法、内置tran_on_batch方法、自定义训练循环。\n",
    "\n",
    "注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import * \n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    today_ts = tf.timestamp()%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8+timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 32\n",
    "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
    "\n",
    "MAX_WORDS = x_train.max()+1\n",
    "CAT_NUM = y_train.max()+1\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
    "   \n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
    "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "          .prefetch(tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 300), dtype=int32, numpy=\n",
      "array([[   0,    0,    0, ..., 1325,   17,   12],\n",
      "       [   0,    0,    0, ...,   67,   17,   12],\n",
      "       [ 126,  174,  247, ...,   16,   17,   12],\n",
      "       ...,\n",
      "       [   0,    0,    0, ...,  272,   17,   12],\n",
      "       [   0,    0,    0, ...,  280,   17,   12],\n",
      "       [   0,    0,    0, ..., 1097,   17,   12]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([ 3,  3, 11,  4,  2, 19, 35, 19,  4, 19,  3,  3, 20,  4, 19,  4,  4,\n",
      "       19,  3,  3,  3,  3,  4, 11,  3,  1,  3,  1, 19,  3,  4,  1])>)\n",
      "(<tf.Tensor: shape=(32, 300), dtype=int32, numpy=\n",
      "array([[   0,    0,    0, ...,   96,   17,   12],\n",
      "       [   0,    0,    0, ..., 2278,   17,   12],\n",
      "       [   0,    0,    0, ...,    8,   17,   12],\n",
      "       ...,\n",
      "       [   0,    0,    0, ...,  152,   17,   12],\n",
      "       [   4, 2397,   51, ...,    8,   17,   12],\n",
      "       [  95,   27,  661, ...,  252,   17,   12]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([ 3,  3,  4, 20, 16,  3, 26,  4,  4,  3, 16,  1,  4,  3,  1, 11,  4,\n",
      "        4,  3, 16,  1,  3, 15, 12, 25, 19,  3, 11, 11,  3,  8, 21])>)\n"
     ]
    }
   ],
   "source": [
    "for item in ds_test.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一，内置方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。\n",
    "\n",
    "并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "281/281 [==============================] - 2s 7ms/step - loss: 2.0304 - sparse_categorical_accuracy: 0.4530 - sparse_top_k_categorical_accuracy: 0.7420 - val_loss: 1.7189 - val_sparse_categorical_accuracy: 0.5516 - val_sparse_top_k_categorical_accuracy: 0.7529\n",
      "Epoch 2/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 1.5096 - sparse_categorical_accuracy: 0.6151 - sparse_top_k_categorical_accuracy: 0.7920 - val_loss: 1.5630 - val_sparse_categorical_accuracy: 0.6118 - val_sparse_top_k_categorical_accuracy: 0.7809\n",
      "Epoch 3/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 1.2275 - sparse_categorical_accuracy: 0.6840 - sparse_top_k_categorical_accuracy: 0.8426 - val_loss: 1.5712 - val_sparse_categorical_accuracy: 0.6247 - val_sparse_top_k_categorical_accuracy: 0.8028\n",
      "Epoch 4/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.9560 - sparse_categorical_accuracy: 0.7506 - sparse_top_k_categorical_accuracy: 0.9016 - val_loss: 1.7297 - val_sparse_categorical_accuracy: 0.6251 - val_sparse_top_k_categorical_accuracy: 0.8063\n",
      "Epoch 5/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.7123 - sparse_categorical_accuracy: 0.8171 - sparse_top_k_categorical_accuracy: 0.9422 - val_loss: 1.9536 - val_sparse_categorical_accuracy: 0.6193 - val_sparse_top_k_categorical_accuracy: 0.8023\n",
      "Epoch 6/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.5340 - sparse_categorical_accuracy: 0.8660 - sparse_top_k_categorical_accuracy: 0.9668 - val_loss: 2.1980 - val_sparse_categorical_accuracy: 0.6202 - val_sparse_top_k_categorical_accuracy: 0.8041\n",
      "Epoch 7/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.4192 - sparse_categorical_accuracy: 0.8960 - sparse_top_k_categorical_accuracy: 0.9792 - val_loss: 2.4595 - val_sparse_categorical_accuracy: 0.6198 - val_sparse_top_k_categorical_accuracy: 0.7996\n",
      "Epoch 8/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.3472 - sparse_categorical_accuracy: 0.9156 - sparse_top_k_categorical_accuracy: 0.9860 - val_loss: 2.6805 - val_sparse_categorical_accuracy: 0.6122 - val_sparse_top_k_categorical_accuracy: 0.8032\n",
      "Epoch 9/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.9255 - sparse_top_k_categorical_accuracy: 0.9900 - val_loss: 2.8674 - val_sparse_categorical_accuracy: 0.6144 - val_sparse_top_k_categorical_accuracy: 0.8054\n",
      "Epoch 10/10\n",
      "281/281 [==============================] - 2s 6ms/step - loss: 0.2681 - sparse_categorical_accuracy: 0.9332 - sparse_top_k_categorical_accuracy: 0.9925 - val_loss: 3.0320 - val_sparse_categorical_accuracy: 0.6149 - val_sparse_top_k_categorical_accuracy: 0.8045\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0e1873fc90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.030376672744751,\n",
       "  1.509628176689148,\n",
       "  1.2274523973464966,\n",
       "  0.9560253024101257,\n",
       "  0.7123274207115173,\n",
       "  0.5339815020561218,\n",
       "  0.4191669821739197,\n",
       "  0.3471987247467041,\n",
       "  0.3004436492919922,\n",
       "  0.2680912911891937],\n",
       " 'sparse_categorical_accuracy': [0.45301714539527893,\n",
       "  0.6151190996170044,\n",
       "  0.6840347647666931,\n",
       "  0.7506123185157776,\n",
       "  0.8170785903930664,\n",
       "  0.8659541010856628,\n",
       "  0.8960142731666565,\n",
       "  0.9156090021133423,\n",
       "  0.9255176782608032,\n",
       "  0.9331997036933899],\n",
       " 'sparse_top_k_categorical_accuracy': [0.7420396208763123,\n",
       "  0.7920284867286682,\n",
       "  0.8425740599632263,\n",
       "  0.9015809297561646,\n",
       "  0.9422177672386169,\n",
       "  0.9668225049972534,\n",
       "  0.9791805744171143,\n",
       "  0.9859719276428223,\n",
       "  0.9899799823760986,\n",
       "  0.9925406575202942],\n",
       " 'val_loss': [1.7188937664031982,\n",
       "  1.5630298852920532,\n",
       "  1.5711621046066284,\n",
       "  1.7296655178070068,\n",
       "  1.9535658359527588,\n",
       "  2.1979970932006836,\n",
       "  2.459477424621582,\n",
       "  2.6805038452148438,\n",
       "  2.867391347885132,\n",
       "  3.0320279598236084],\n",
       " 'val_sparse_categorical_accuracy': [0.5516473650932312,\n",
       "  0.6117542386054993,\n",
       "  0.6246660947799683,\n",
       "  0.6251112818717957,\n",
       "  0.6193232536315918,\n",
       "  0.6202136874198914,\n",
       "  0.619768500328064,\n",
       "  0.6121994853019714,\n",
       "  0.6144256591796875,\n",
       "  0.6148709058761597],\n",
       " 'val_sparse_top_k_categorical_accuracy': [0.7528940439224243,\n",
       "  0.7809438705444336,\n",
       "  0.8027604818344116,\n",
       "  0.8063223361968994,\n",
       "  0.8023152351379395,\n",
       "  0.8040961623191833,\n",
       "  0.7996438145637512,\n",
       "  0.8032057285308838,\n",
       "  0.8054319024085999,\n",
       "  0.8045414090156555]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二，内置train_on_batch方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=optimizers.Nadam(),\n",
    "                loss=losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n",
    "    return(model)\n",
    " \n",
    "model = create_model()\n",
    "model.summary()\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,ds_train,ds_valid,epoches):\n",
    "\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # 在后期降低学习率\n",
    "        if epoch == 5:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "        \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "\n",
    "        for x, y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
    "            \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
    "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================11:42:14\n",
      "epoch =  1\n",
      "train: {'loss': 2.1657514572143555, 'sparse_categorical_accuracy': 0.5454545617103577, 'sparse_top_k_categorical_accuracy': 0.7272727489471436}\n",
      "valid: {'loss': 1.706920862197876, 'sparse_categorical_accuracy': 0.5529831051826477, 'sparse_top_k_categorical_accuracy': 0.7569011449813843}\n",
      "\n",
      "================================================================================11:42:16\n",
      "epoch =  2\n",
      "train: {'loss': 1.8134582042694092, 'sparse_categorical_accuracy': 0.5454545617103577, 'sparse_top_k_categorical_accuracy': 0.7272727489471436}\n",
      "valid: {'loss': 1.550385594367981, 'sparse_categorical_accuracy': 0.6032947301864624, 'sparse_top_k_categorical_accuracy': 0.7796081900596619}\n",
      "\n",
      "================================================================================11:42:19\n",
      "epoch =  3\n",
      "train: {'loss': 1.4261715412139893, 'sparse_categorical_accuracy': 0.5, 'sparse_top_k_categorical_accuracy': 0.8181818127632141}\n",
      "valid: {'loss': 1.5552479028701782, 'sparse_categorical_accuracy': 0.6402493119239807, 'sparse_top_k_categorical_accuracy': 0.8040961623191833}\n",
      "\n",
      "================================================================================11:42:21\n",
      "epoch =  4\n",
      "train: {'loss': 1.0415945053100586, 'sparse_categorical_accuracy': 0.6818181872367859, 'sparse_top_k_categorical_accuracy': 0.9090909361839294}\n",
      "valid: {'loss': 1.7367298603057861, 'sparse_categorical_accuracy': 0.6357969641685486, 'sparse_top_k_categorical_accuracy': 0.8063223361968994}\n",
      "\n",
      "Lowering optimizer Learning Rate...\n",
      "\n",
      "\n",
      "================================================================================11:42:24\n",
      "epoch =  5\n",
      "train: {'loss': 0.6475916504859924, 'sparse_categorical_accuracy': 0.7272727489471436, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 1.9818546772003174, 'sparse_categorical_accuracy': 0.6317898631095886, 'sparse_top_k_categorical_accuracy': 0.8032057285308838}\n",
      "\n",
      "================================================================================11:42:27\n",
      "epoch =  6\n",
      "train: {'loss': 0.5329045653343201, 'sparse_categorical_accuracy': 0.8181818127632141, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.1635286808013916, 'sparse_categorical_accuracy': 0.6331255435943604, 'sparse_top_k_categorical_accuracy': 0.8067675828933716}\n",
      "\n",
      "================================================================================11:42:30\n",
      "epoch =  7\n",
      "train: {'loss': 0.44471463561058044, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.338655710220337, 'sparse_categorical_accuracy': 0.6317898631095886, 'sparse_top_k_categorical_accuracy': 0.8107746839523315}\n",
      "\n",
      "================================================================================11:42:33\n",
      "epoch =  8\n",
      "train: {'loss': 0.3826679289340973, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.491145133972168, 'sparse_categorical_accuracy': 0.6304541230201721, 'sparse_top_k_categorical_accuracy': 0.809884250164032}\n",
      "\n",
      "================================================================================11:42:35\n",
      "epoch =  9\n",
      "train: {'loss': 0.34594959020614624, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.607635498046875, 'sparse_categorical_accuracy': 0.6264470219612122, 'sparse_top_k_categorical_accuracy': 0.8103294968605042}\n",
      "\n",
      "================================================================================11:42:38\n",
      "epoch =  10\n",
      "train: {'loss': 0.3100031912326813, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
      "valid: {'loss': 2.7352585792541504, 'sparse_categorical_accuracy': 0.6251112818717957, 'sparse_top_k_categorical_accuracy': 0.8103294968605042}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model,ds_train,ds_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三，自定义训练循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 7)            216874    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2336)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                107502    \n",
      "=================================================================\n",
      "Total params: 332,856\n",
      "Trainable params: 332,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "    model.add(layers.MaxPool1D(2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
    "    return(model)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================11:46:18\n",
      "Epoch=1,Loss:2.01347947,Accuracy:0.46938321,Valid Loss:1.67509925,Valid Accuracy:0.564114\n",
      "\n",
      "================================================================================11:46:20\n",
      "Epoch=2,Loss:1.48007202,Accuracy:0.61445111,Valid Loss:1.54901266,Valid Accuracy:0.606411397\n",
      "\n",
      "================================================================================11:46:22\n",
      "Epoch=3,Loss:1.19723749,Accuracy:0.687931418,Valid Loss:1.57189929,Valid Accuracy:0.630454123\n",
      "\n",
      "================================================================================11:46:24\n",
      "Epoch=4,Loss:0.936013877,Accuracy:0.754620373,Valid Loss:1.741889,Valid Accuracy:0.626892269\n",
      "\n",
      "================================================================================11:46:25\n",
      "Epoch=5,Loss:0.709292531,Accuracy:0.815185905,Valid Loss:1.98559523,Valid Accuracy:0.625111282\n",
      "\n",
      "================================================================================11:46:27\n",
      "Epoch=6,Loss:0.53942579,Accuracy:0.863950133,Valid Loss:2.23997688,Valid Accuracy:0.61843276\n",
      "\n",
      "================================================================================11:46:29\n",
      "Epoch=7,Loss:0.428467482,Accuracy:0.89723891,Valid Loss:2.49434376,Valid Accuracy:0.616206586\n",
      "\n",
      "================================================================================11:46:30\n",
      "Epoch=8,Loss:0.35636577,Accuracy:0.915943,Valid Loss:2.75245285,Valid Accuracy:0.614425659\n",
      "\n",
      "================================================================================11:46:32\n",
      "Epoch=9,Loss:0.307280302,Accuracy:0.925963044,Valid Loss:2.96957374,Valid Accuracy:0.610418499\n",
      "\n",
      "================================================================================11:46:33\n",
      "Epoch=10,Loss:0.271974206,Accuracy:0.934090376,Valid Loss:3.17662549,Valid Accuracy:0.605520904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model,ds_train,ds_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
